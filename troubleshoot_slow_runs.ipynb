{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/ML/TRAIN/jax/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Transposing  lm_head\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 8, 50257)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flax.nnx as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "q = \"Hello, I'm a language model,\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "from jax_gpt2 import GPT, GPTConfig\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "logits = model(tokens)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "q = \"Hello, I'm a language model,\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "from jax_gpt2 import GPT, GPTConfig\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "logits = model(tokens)\n",
    "\n",
    "while tokens.shape[1] < 10: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    logits = model(tokens) # (B, T, vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turns out I was ridiculously running a forever loop, tokens.shape[1] never got updated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Transposing  lm_head\n",
      "> Hello, I'm a language model, in some of the other parts of the good news at the 'We need the very common use the time and\n",
      "> Hello, I'm a language model, \"No less two. They had in the world. After the one thing it is also the former year.\n",
      "> Hello, I'm a language model, on the world are the way one of the city (D. They are not be more than and in the\n",
      "> Hello, I'm a language model, in a good news such far or with a big deal of the time of CUD.\n",
      "The first to\n",
      "> Hello, I'm a language model, or the current government will be able to put the same or,\n",
      "\n",
      "This last few other big big things\n"
     ]
    }
   ],
   "source": [
    "import flax.nnx as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "q = \"Hello, I'm a language model,\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "from jax_gpt2 import GPT, GPTConfig\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "step_key = jax.random.key(0)\n",
    "\n",
    "while tokens.shape[1] < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    logits = model(tokens) # (B, T, vocab_size) \n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :] # (B, vocab_size)\n",
    "    # get the probabilities\n",
    "    probs = nn.softmax(logits, axis=-1)\n",
    "    # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "    top_logits, top_tokens = jax.lax.top_k(logits, min(50, logits.shape[-1]))\n",
    "    step_key, subkey = jax.random.split(step_key)\n",
    "    token_idx = jax.random.categorical(step_key, top_logits, axis=-1)\n",
    "    next_token = jnp.take_along_axis(top_tokens, token_idx[:, None], axis=-1).squeeze(-1)\n",
    "    tokens = jnp.concatenate((tokens, jnp.vstack(next_token)), axis=1)\n",
    "    # print(f\"Updated value of tokens.shape[1]: {tokens.shape[1]}\")\n",
    "\n",
    "# print the generated text\n",
    "\n",
    "for i in range(5):\n",
    "    x = tokens[i, :30].tolist()\n",
    "    decoded = enc.decode(x)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowness(Actually infinite loop) fixed, but still jibberish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
