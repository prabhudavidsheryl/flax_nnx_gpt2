{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/ML/TRAIN/jax/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Rowan/hellaswag\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def render_example(example):\n",
    "    \"\"\"\n",
    "    Given the example as a dictionary, render it as three torch tensors:\n",
    "    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
    "    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
    "    - label (the index of the correct completion, which we hope has the highest likelihood)\n",
    "    \"\"\"\n",
    "    ctx = example[\"ctx\"]\n",
    "    label = example[\"label\"]\n",
    "    endings = example[\"endings\"]\n",
    "\n",
    "    # data needed to reproduce this eval on the C size\n",
    "    data = {\n",
    "        \"label\": label,\n",
    "        \"ctx_tokens\": None,\n",
    "        \"ending_tokens\": [],\n",
    "    }\n",
    "\n",
    "    # gather up all the tokens\n",
    "    ctx_tokens = enc.encode(ctx)\n",
    "    data[\"ctx_tokens\"] = ctx_tokens\n",
    "    tok_rows = []\n",
    "    mask_rows = []\n",
    "\n",
    "    # print(f\"Context tokens: {ctx}\")\n",
    "    # print(f\"Endings: {endings}\")\n",
    "\n",
    "    for end in endings:\n",
    "        end_tokens = enc.encode(\" \" + end) # note: prepending \" \" because GPT-2 tokenizer\n",
    "        tok_rows.append(ctx_tokens + end_tokens)\n",
    "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "        data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "    # have to be careful during the collation because the number of tokens in each row can differ\n",
    "    max_len = max(len(row) for row in tok_rows)\n",
    "    tokens = jnp.zeros((4, max_len), dtype=jnp.int32)\n",
    "    mask = jnp.zeros((4, max_len))\n",
    "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "        tokens = tokens.at[i, :len(tok_row)].set(jnp.array(tok_row))\n",
    "        mask = mask.at[i, :len(mask_row)].set(jnp.array(mask_row))\n",
    "\n",
    "    return data, tokens, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 2., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.zeros((4,3))\n",
    "x = x.at[3, :2].set(jnp.array([1,2]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 20, 50257)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel\n",
    "model = FlaxGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "data, tokens, mask, label = render_example(dataset[0])\n",
    "logits = model(tokens).logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19, 50257)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[..., :-1, :].shape\n",
    "tokens.shape\n",
    "tokens[..., 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120721/117920400.py:3: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  x = torch.from_numpy(np.asarray(logits[..., :-1, :]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 19, 50257])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([76, 50257])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 19])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([76])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.from_numpy(np.asarray(logits[..., :-1, :]))\n",
    "x.shape\n",
    "#x.size(-1)\n",
    "x.view(-1, x.size(-1)).shape    # -1 allows 76 to be inferred\n",
    "y = torch.from_numpy(np.asarray(tokens[..., 1:]))\n",
    "y.shape\n",
    "y.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 50257)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(76,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits = logits[..., :-1, :]\n",
    "shift_tokens = tokens[..., 1:]\n",
    "shift_logits.reshape([-1, shift_logits.shape[-1]]).shape\n",
    "shift_tokens.reshape([-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-33.570633, -32.76895 , -35.450985, ..., -40.98076 , -40.18672 ,\n",
       "        -33.21528 ], dtype=float32),\n",
       " Array(582, dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(76,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Array(7.302154, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optax\n",
    "flat_shift_logits = shift_logits.reshape([-1, shift_logits.shape[-1]])\n",
    "flat_shift_tokens = shift_tokens.reshape([-1])\n",
    "flat_shift_logits[0], jnp.int32(flat_shift_tokens)[0]\n",
    "shift_losses = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    flat_shift_logits, \n",
    "    jnp.int32(flat_shift_tokens))\n",
    "shift_losses.shape\n",
    "shift_losses[0]\n",
    "shift_losses.reshape(tokens.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([7.3021541e+00, 3.3172197e+00, 6.1763878e+00, 1.3953547e+00,\n",
       "       8.9701486e-01, 5.5496802e+00, 3.7710347e+00, 7.8738132e+00,\n",
       "       1.5087028e+00, 5.8585443e+00, 1.1923371e+01, 2.1637678e+00,\n",
       "       2.7706001e+00, 2.6047549e+00, 5.8060970e+00, 1.1142294e-02,\n",
       "       7.5942540e+00, 1.6238186e+00, 1.5568147e+00], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_losses.reshape(tokens.shape[0], -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Array([43.421867, 34.639187, 23.588688, 36.63145 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Array([3.9474425, 5.7731977, 2.948586 , 4.0701613], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_mask = mask[..., 1:] # we must shift mask, so we start at the last prompt token\n",
    "masked_shift_losses = shift_losses.reshape(tokens.shape[0], -1) * shift_mask\n",
    "masked_shift_losses.shape\n",
    "shift_mask.shape\n",
    "# sum and divide by the number of 1s in the mask\n",
    "sum_loss = masked_shift_losses.sum(axis=1)\n",
    "avg_loss = sum_loss / shift_mask.sum(axis=1)\n",
    "sum_loss\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_loss.argmin().item()\n",
    "avg_loss.argmin().item()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model = FlaxGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    for example in dataset:\n",
    "        data, tokens, mask, label = render_example(example)\n",
    "        # get the logits\n",
    "        logits = model(tokens).logits\n",
    "        # evaluate the autoregressive loss at all positions\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_tokens = tokens[..., 1:]\n",
    "        flat_shift_logits = shift_logits.reshape([-1, shift_logits.shape[-1]])\n",
    "        flat_shift_tokens = shift_tokens.reshape([-1])\n",
    "        shift_losses = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            flat_shift_logits, \n",
    "            jnp.int32(flat_shift_tokens)\n",
    "            )\n",
    "        shift_losses = shift_losses.reshape(tokens.shape[0], -1)\n",
    "        # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "        shift_mask = mask[..., 1:] # we must shift mask, so we start at the last prompt token\n",
    "        masked_shift_losses = shift_losses * shift_mask\n",
    "        # sum and divide by the number of 1s in the mask\n",
    "        sum_loss = masked_shift_losses.sum(axis=1)\n",
    "        avg_loss = sum_loss / shift_mask.sum(axis=1)\n",
    "        # now we have a loss for each of the 4 completions\n",
    "        # the one with the lowest loss should be the most likely\n",
    "        pred = sum_loss.argmin().item()\n",
    "        pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "        # accumulate stats\n",
    "        num_total += 1\n",
    "        num_correct += int(pred == int(label))\n",
    "        num_correct_norm += int(pred_norm == int(label))\n",
    "        print(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "        # debug: pretty print a few examples, and the losses in each case\n",
    "        if num_total < 10:\n",
    "            print(\"---\")\n",
    "            print(f\"Context:\\n {example['ctx']}\")\n",
    "            print(f\"Endings:\")\n",
    "            for i, end in enumerate(example[\"endings\"]):\n",
    "                print(f\"{i} (loss: {avg_loss[i].item():.4f}) {end}\")\n",
    "            print(f\"predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "        if num_total > 51:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 acc_norm: 0/1=0.0000\n",
      "---\n",
      "Context:\n",
      " A man is sitting on a roof. he\n",
      "Endings:\n",
      "0 (loss: 3.9474) is using wrap to wrap a pair of skis.\n",
      "1 (loss: 5.7732) is ripping level tiles off.\n",
      "2 (loss: 2.9486) is holding a rubik's cube.\n",
      "3 (loss: 4.0702) starts pulling up roofing on a roof.\n",
      "predicted: 2, actual: 3\n",
      "2 acc_norm: 0/2=0.0000\n",
      "---\n",
      "Context:\n",
      " A lady walks to a barbell. She bends down and grabs the pole. the lady\n",
      "Endings:\n",
      "0 (loss: 3.6541) swings and lands in her arms.\n",
      "1 (loss: 2.5001) pulls the barbell forward.\n",
      "2 (loss: 2.3630) pulls a rope attached to the barbell.\n",
      "3 (loss: 2.8399) stands and lifts the weight over her head.\n",
      "predicted: 2, actual: 3\n",
      "3 acc_norm: 1/3=0.3333\n",
      "---\n",
      "Context:\n",
      " Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man\n",
      "Endings:\n",
      "0 (loss: 2.9822) are then shown paddling down a river in a boat while a woman talks.\n",
      "1 (loss: 3.3812) are driving the canoe, they go down the river flowing side to side.\n",
      "2 (loss: 2.1979) sit in a canoe while the man paddles.\n",
      "3 (loss: 4.4141) walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
      "predicted: 2, actual: 2\n",
      "4 acc_norm: 1/4=0.2500\n",
      "---\n",
      "Context:\n",
      " A boy is running down a track. the boy\n",
      "Endings:\n",
      "0 (loss: 2.4140) runs into a car.\n",
      "1 (loss: 4.5476) gets in a mat.\n",
      "2 (loss: 3.6283) lifts his body above the height of a pole.\n",
      "3 (loss: 4.5574) stands on his hands and springs.\n",
      "predicted: 0, actual: 2\n",
      "5 acc_norm: 1/5=0.2000\n",
      "---\n",
      "Context:\n",
      " The boy lifts his body above the height of a pole. The boy lands on his back on to a red mat. the boy\n",
      "Endings:\n",
      "0 (loss: 2.1563) turns his body around on the mat.\n",
      "1 (loss: 1.9198) gets up from the mat.\n",
      "2 (loss: 1.7635) continues to lift his body over the pole.\n",
      "3 (loss: 2.1991) wiggles out of the mat.\n",
      "predicted: 2, actual: 1\n",
      "6 acc_norm: 1/6=0.1667\n",
      "---\n",
      "Context:\n",
      " The boy lands on his back on to a red mat. The boy gets up from the mat. the boy\n",
      "Endings:\n",
      "0 (loss: 4.8307) starts doing spins.\n",
      "1 (loss: 3.8748) celebrates by clapping and flexing both arms.\n",
      "2 (loss: 2.4393) is dancing on the mat.\n",
      "3 (loss: 4.8177) does jump jacks on his stick.\n",
      "predicted: 2, actual: 1\n",
      "7 acc_norm: 1/7=0.1429\n",
      "---\n",
      "Context:\n",
      " A man is standing in front of a camera. He starts playing a harmonica for the camera. he\n",
      "Endings:\n",
      "0 (loss: 2.1624) begins to play the harmonica with his body while looking at the camera.\n",
      "1 (loss: 2.2924) seems to be singing while playing the harmonica.\n",
      "2 (loss: 2.5183) rocks back and forth to the music as he goes.\n",
      "3 (loss: 3.0957) painted a fence in front of the camera.\n",
      "predicted: 0, actual: 2\n",
      "8 acc_norm: 1/8=0.1250\n",
      "---\n",
      "Context:\n",
      " A cartoon animation video is shown with people wandering around and rockets being shot. two men\n",
      "Endings:\n",
      "0 (loss: 5.4415) fight robots of evil and ends with a to be continued.\n",
      "1 (loss: 4.2968) are then shown in closeups shooting a shot put.\n",
      "2 (loss: 3.6007) push a child in a speedboat in the water.\n",
      "3 (loss: 3.0403) look in the cameraman's eye and smile.\n",
      "predicted: 3, actual: 0\n",
      "9 acc_norm: 1/9=0.1111\n",
      "---\n",
      "Context:\n",
      " A man is holding a pocket knife while sitting on some rocks in the wilderness. then he\n",
      "Endings:\n",
      "0 (loss: 3.7924) opens a can of oil put oil on the knife, and puts oil on a knife and press it through a can filled with oil then cuts several pieces from the sandwiches.\n",
      "1 (loss: 3.3187) takes a small stone from the flowing river and smashes it on another stone.\n",
      "2 (loss: 3.0227) uses the knife to shave his leg.\n",
      "3 (loss: 5.4330) sand the rocks and tops them by using strong pressure.\n",
      "predicted: 2, actual: 1\n",
      "10 acc_norm: 1/10=0.1000\n",
      "11 acc_norm: 2/11=0.1818\n",
      "12 acc_norm: 2/12=0.1667\n",
      "13 acc_norm: 2/13=0.1538\n",
      "14 acc_norm: 2/14=0.1429\n",
      "15 acc_norm: 3/15=0.2000\n",
      "16 acc_norm: 3/16=0.1875\n",
      "17 acc_norm: 3/17=0.1765\n",
      "18 acc_norm: 3/18=0.1667\n",
      "19 acc_norm: 3/19=0.1579\n",
      "20 acc_norm: 3/20=0.1500\n",
      "21 acc_norm: 3/21=0.1429\n",
      "22 acc_norm: 3/22=0.1364\n",
      "23 acc_norm: 4/23=0.1739\n",
      "24 acc_norm: 4/24=0.1667\n",
      "25 acc_norm: 4/25=0.1600\n",
      "26 acc_norm: 4/26=0.1538\n",
      "27 acc_norm: 5/27=0.1852\n",
      "28 acc_norm: 6/28=0.2143\n",
      "29 acc_norm: 7/29=0.2414\n",
      "30 acc_norm: 7/30=0.2333\n",
      "31 acc_norm: 7/31=0.2258\n",
      "32 acc_norm: 7/32=0.2188\n",
      "33 acc_norm: 8/33=0.2424\n",
      "34 acc_norm: 8/34=0.2353\n",
      "35 acc_norm: 8/35=0.2286\n",
      "36 acc_norm: 8/36=0.2222\n",
      "37 acc_norm: 9/37=0.2432\n",
      "38 acc_norm: 9/38=0.2368\n",
      "39 acc_norm: 10/39=0.2564\n",
      "40 acc_norm: 11/40=0.2750\n",
      "41 acc_norm: 11/41=0.2683\n",
      "42 acc_norm: 11/42=0.2619\n",
      "43 acc_norm: 12/43=0.2791\n",
      "44 acc_norm: 13/44=0.2955\n",
      "45 acc_norm: 13/45=0.2889\n",
      "46 acc_norm: 14/46=0.3043\n",
      "47 acc_norm: 15/47=0.3191\n",
      "48 acc_norm: 15/48=0.3125\n",
      "49 acc_norm: 15/49=0.3061\n",
      "50 acc_norm: 16/50=0.3200\n",
      "51 acc_norm: 16/51=0.3137\n",
      "52 acc_norm: 17/52=0.3269\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Total JAX matrices: 149\n",
      "Transposing  lm_head\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(32, dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax_gpt2 import GPT\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "data, tokens, mask, label = render_example(dataset[0])\n",
    "tokens[0][0]\n",
    "tokens.shape\n",
    "logits = model(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
