{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/ML/TRAIN/jax/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Transposing  lm_head\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  config=GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),\n",
       "  wte=Embed(\n",
       "    embedding=Param(\n",
       "      value=Array(shape=(50257, 768), dtype=float32)\n",
       "    ),\n",
       "    num_embeddings=50257,\n",
       "    features=768,\n",
       "    dtype=dtype('float32'),\n",
       "    param_dtype=<class 'jax.numpy.float32'>,\n",
       "    embedding_init=<function variance_scaling.<locals>.init at 0x7f7b84c03e50>\n",
       "  ),\n",
       "  wpe=Embed(\n",
       "    embedding=Param(\n",
       "      value=Array(shape=(1024, 768), dtype=float32)\n",
       "    ),\n",
       "    num_embeddings=1024,\n",
       "    features=768,\n",
       "    dtype=dtype('float32'),\n",
       "    param_dtype=<class 'jax.numpy.float32'>,\n",
       "    embedding_init=<function variance_scaling.<locals>.init at 0x7f7b84c03e50>\n",
       "  ),\n",
       "  h=[Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  ), Block(\n",
       "    ln_1=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    attn=CausalSelfAttention(\n",
       "      c_attn=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 2304), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(2304,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=2304,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      n_head=12,\n",
       "      n_embd=768\n",
       "    ),\n",
       "    ln_2=LayerNorm(\n",
       "      scale=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(768,), dtype=float32)\n",
       "      ),\n",
       "      num_features=768,\n",
       "      epsilon=1e-05,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      use_bias=True,\n",
       "      use_scale=True,\n",
       "      bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "      scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "      reduction_axes=-1,\n",
       "      feature_axes=-1,\n",
       "      axis_name=None,\n",
       "      axis_index_groups=None,\n",
       "      use_fast_variance=False\n",
       "    ),\n",
       "    mlp=MLP(\n",
       "      c_fc=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(768, 3072), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(3072,), dtype=float32)\n",
       "        ),\n",
       "        in_features=768,\n",
       "        out_features=3072,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      ),\n",
       "      c_proj=Linear(\n",
       "        kernel=Param(\n",
       "          value=Array(shape=(3072, 768), dtype=float32)\n",
       "        ),\n",
       "        bias=Param(\n",
       "          value=Array(shape=(768,), dtype=float32)\n",
       "        ),\n",
       "        in_features=3072,\n",
       "        out_features=768,\n",
       "        use_bias=True,\n",
       "        dtype=None,\n",
       "        param_dtype=<class 'jax.numpy.float32'>,\n",
       "        precision=None,\n",
       "        kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "        bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "        dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "      )\n",
       "    )\n",
       "  )],\n",
       "  ln_f=LayerNorm(\n",
       "    scale=Param(\n",
       "      value=Array(shape=(768,), dtype=float32)\n",
       "    ),\n",
       "    bias=Param(\n",
       "      value=Array(shape=(768,), dtype=float32)\n",
       "    ),\n",
       "    num_features=768,\n",
       "    epsilon=1e-05,\n",
       "    dtype=None,\n",
       "    param_dtype=<class 'jax.numpy.float32'>,\n",
       "    use_bias=True,\n",
       "    use_scale=True,\n",
       "    bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "    scale_init=<function ones at 0x7f7b8542ee50>,\n",
       "    reduction_axes=-1,\n",
       "    feature_axes=-1,\n",
       "    axis_name=None,\n",
       "    axis_index_groups=None,\n",
       "    use_fast_variance=False\n",
       "  ),\n",
       "  lm_head=Linear(\n",
       "    kernel=Param(\n",
       "      value=Array(shape=(768, 50257), dtype=float32)\n",
       "    ),\n",
       "    bias=Param(\n",
       "      value=None\n",
       "    ),\n",
       "    in_features=768,\n",
       "    out_features=50257,\n",
       "    use_bias=False,\n",
       "    dtype=None,\n",
       "    param_dtype=<class 'jax.numpy.float32'>,\n",
       "    precision=None,\n",
       "    kernel_init=<function variance_scaling.<locals>.init at 0x7f7b84c03b80>,\n",
       "    bias_init=<function zeros at 0x7f7b8542eca0>,\n",
       "    dot_general=<function dot_general at 0x7f7b85a2e310>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax_gpt2 import GPT, GPTConfig\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1, 2, 50257)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "type(jnp.array([[10,11]]))\n",
    "jnp.array([[10,11]]).shape\n",
    "\n",
    "model(jnp.array([[10,11]])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model load success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "q = \"Hello, I'm a language model,\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 50257)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(tokens)\n",
    "preds.shape\n",
    "preds.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impatient generate attempt by converting JAX model outputs to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/ML/TRAIN/jax/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Transposing  lm_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88416/254563860.py:44: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tokens = torch.from_numpy(np.asarray(tokens))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, you want to work the first one important time,\n",
      "With I'm't only 1-level A 'R\n",
      "> Hello, I'm a language model, that you can't be asked this a big difference or the latest to the most time, he won't ask\n",
      "> Hello, I'm a language model, we must be\n",
      "I's now in the current and an final final, for more than The two. It\n",
      "> Hello, I'm a language model, (This is still need to try, the same thing like in a good news in that may take them \"\n",
      "> Hello, I'm a language model, they could then have to the very small number of of 'In his new in the most a few days after\n"
     ]
    }
   ],
   "source": [
    "import flax.nnx as nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "q = \"Hello, I'm a language model,\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "from jax_gpt2 import GPT, GPTConfig\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# q = \"Hello, I'm a language model,\"\n",
    "# tokens = tokenizer.encode(q)\n",
    "# tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "# tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "while tokens.shape[1] < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    logits = model(tokens) # (B, T, vocab_size) \n",
    "    logits = torch.from_numpy(np.array(logits))\n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :] # (B, vocab_size)\n",
    "    # get the probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "    # select a token from the top-k probabilities\n",
    "    # note: multinomial does not demand the input to sum to 1\n",
    "    ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "    # gather the corresponding indices\n",
    "    xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "    # append to the sequence\n",
    "    tokens = torch.from_numpy(np.asarray(tokens))\n",
    "    tokens = torch.cat((tokens, xcol), dim=1)\n",
    "    tokens = jnp.array(tokens.cpu().numpy())\n",
    "\n",
    "# print the generated text\n",
    "\n",
    "tokens = torch.from_numpy(np.array(tokens))\n",
    "for i in range(5):\n",
    "    x = tokens[i, :30].tolist()\n",
    "    decoded = enc.decode(x)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argmax attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, the same, the same, the same, the same, the same, the same, the same, the\n",
      "> Hello, I'm a language model, the same, the same, the same, the same, the same, the same, the same, the\n",
      "> Hello, I'm a language model, the same, the same, the same, the same, the same, the same, the same, the\n",
      "> Hello, I'm a language model, the same, the same, the same, the same, the same, the same, the same, the\n",
      "> Hello, I'm a language model, the same, the same, the same, the same, the same, the same, the same, the\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "q = \"Hello, I'm a language model,\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "while tokens.shape[1] < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    logits = model(tokens) # (B, T, vocab_size) \n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :] # (B, vocab_size)\n",
    "    # get the probabilities\n",
    "    probs = nn.softmax(logits, axis=-1)\n",
    "    amax = probs.argmax(axis=-1)\n",
    "    tokens = jnp.concatenate((tokens, jnp.vstack(amax)), axis=1)\n",
    "\n",
    "# print the generated text\n",
    "\n",
    "for i in range(5):\n",
    "    x = tokens[i, :30].tolist()\n",
    "    decoded = enc.decode(x)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f426f0da290>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\n",
      "> Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\n",
      "> Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\n",
      "> Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\n",
      "> Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "torch.manual_seed(42)\n",
    "tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \"Hello, I'm a language model,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "x = tokens\n",
    "\n",
    "# generate!\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        amax = probs.argmax(dim=-1).reshape(x.size(0),1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, amax), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a Pytorch GPT2, does it really work? It does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f426f0da290>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, not a programming platform! I just make decisions based on other projects. I try to do that.\"\n",
      "\n",
      "\n",
      "> Hello, I'm a language model, a kind of a \"first class citizen\" of the world and a person that comes from a much more egalitarian\n",
      "> Hello, I'm a language model, and I'm starting to talk about the notion of the syntax, and I'm also working on an extension that\n",
      "> Hello, I'm a language model, because I'm writing real-time. I'm writing all languages. And I'm working with languages for me\n",
      "> Hello, I'm a language model, I don't know where to begin but I know there is a big deal going on with our society. What\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "torch.manual_seed(42)\n",
    "tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \"Hello, I'm a language model,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "x = tokens\n",
    "\n",
    "# generate!\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it a datatype thing? No by the looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") \n",
    "model.state_dict()[\"transformer.wte.weight\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried a pure JAX generate and ran into a forever loop!! Thought it was slowness!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing in troubleshoot_gibberish.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
