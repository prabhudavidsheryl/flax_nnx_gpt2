{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Total JAX matrices: 149\n",
      "Transposing  lm_head\n",
      "> Capital of India is set to launch its first full-service Indian bank service next month.\n",
      "\n",
      "The bank, started in 2007, will provide loans\n",
      "> Capital of India is on a collision course with Pakistan to ensure that it will not be forced to defend the rights of its citizens with weapons. It continues\n",
      "> Capital of India is not a government funded company, and its executives are private. The company has just received money by selling shares in two state companies that\n",
      "> Capital of India is now expanding ahead of the financial year in February, according to a regulatory report released today. According to the report, India's growth\n",
      "> Capital of India is an integral step in our country's economic prosperity and has also helped drive India's economic development in several areas, such as investment,\n"
     ]
    }
   ],
   "source": [
    "import flax.nnx as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# q = \"Hello, I'm a language model,\"\n",
    "q = \"Capital of India is\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "from jax_gpt2 import GPT, GPTConfig\n",
    "# model = GPT.from_pretrained_flax('gpt2')\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "step_key = jax.random.key(0)\n",
    "\n",
    "while tokens.shape[1] < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    logits = model(tokens) # (B, T, vocab_size) \n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :] # (B, vocab_size)\n",
    "    # get the probabilities\n",
    "    # probs = nn.softmax(logits, axis=-1)   # This softmax causes poor generations\n",
    "    # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "    top_logits, top_tokens = jax.lax.top_k(logits, min(50, logits.shape[-1]))\n",
    "    step_key, subkey = jax.random.split(step_key)\n",
    "    token_idx = jax.random.categorical(subkey, top_logits, axis=-1)\n",
    "    next_token = jnp.take_along_axis(top_tokens, token_idx[:, None], axis=-1).squeeze(-1)\n",
    "    tokens = jnp.concatenate((tokens, jnp.vstack(next_token)), axis=1)\n",
    "    # print(f\"Updated value of tokens.shape[1]: {tokens.shape[1]}\")\n",
    "\n",
    "# print the generated text\n",
    "\n",
    "for i in range(5):\n",
    "    x = tokens[i, :30].tolist()\n",
    "    decoded = enc.decode(x)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues faced\n",
    "\n",
    "* Initially loading Huggingface Pytorch weights itself would not work\n",
    "* Ran into an infinite loop and thought that was a slowness issue due to JAX! The reason was that tokens.shape was not getting updated in the generate loop.\n",
    "* After much struggle realized that I was returning x instead of the processed y in the Self Attention code!\n",
    "* And Attention code from https://github.com/cgarciae/nanoGPT-jax and https://github.com/jenkspt/gpt-jax helped get a first working version\n",
    "* Attention version with dot_product_attention and make_causal_mask from flax.nnx was fixed next by correcting the transpose.\n",
    "* The version with weights from HF Flax was still not working. Model result comparison code in GPT._compare was helpful finding that the issue was in the Attention block.\n",
    "* The problem was a missing transpose when loading weights - attn.c_proj "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
