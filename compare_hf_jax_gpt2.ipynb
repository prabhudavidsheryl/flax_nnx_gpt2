{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the generation code using huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Capital of India is also developing a program called AOID.\"AOID is a unique approach to digital rights management. It will become available to\n",
      "> Capital of India is not a sovereign or democratic government. A sovereign or democratically elected government is not a sovereign state or democratic corporation.\n",
      "\n",
      "India has\n",
      "> Capital of India is facing rising cost pressures and rising borrowing costs, and an ever-shrinking middle class. It seems clear that Narendra Modi is one\n",
      "> Capital of India is also the most expensive financial institution we have seen in our history. Even if BIC had been in the financial sector in 1989,\n",
      "> Capital of India is currently in transition to create a more diversified economy. India's growth has been very sluggish since the middle of 2007 but its share\n"
     ]
    }
   ],
   "source": [
    "import flax.nnx as nn\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# q = \"Hello, I'm a language model,\"\n",
    "q = \"Capital of India is\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "tokens = jnp.repeat(tokens, 5, axis=0)\n",
    "\n",
    "from transformers import GPT2Tokenizer, FlaxGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = FlaxGPT2Model.from_pretrained('gpt2')\n",
    "model = FlaxGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "step_key = jax.random.key(0)\n",
    "\n",
    "while tokens.shape[1] < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    logits = model(tokens).logits # (B, T, vocab_size) \n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :] # (B, vocab_size)\n",
    "    # get the probabilities\n",
    "    probs = nn.softmax(logits, axis=-1)\n",
    "    # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "    # top_logits, top_tokens = jax.lax.top_k(logits, min(50, logits.shape[-1]))\n",
    "    top_logits, top_tokens = jax.lax.top_k(logits, min(50, logits.shape[-1]))\n",
    "    step_key, subkey = jax.random.split(step_key)\n",
    "    token_idx = jax.random.categorical(step_key, top_logits, axis=-1)\n",
    "    next_token = jnp.take_along_axis(top_tokens, token_idx[:, None], axis=-1).squeeze(-1)\n",
    "    tokens = jnp.concatenate((tokens, jnp.vstack(next_token)), axis=1)\n",
    "    # print(f\"Updated value of tokens.shape[1]: {tokens.shape[1]}\")\n",
    "\n",
    "# print the generated text\n",
    "\n",
    "for i in range(5):\n",
    "    x = tokens[i, :30].tolist()\n",
    "    decoded = enc.decode(x)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ffb107b18d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Capital of India is very interested in providing its own private development project. It took nearly a year or longer before he announced the acquisition of TPG.\n",
      "> Capital of India is under siege, and its financial losses have exceeded Rs1.8 billion. In response, the US Treasury has asked all major states\n",
      "> Capital of India is India's largest investment bank. In fact, it has nearly all of India's major cities. In 2012-13, a total\n",
      "> Capital of India is on the verge of defaulting on its debt, which it believes is too high.<|endoftext|>Gardiner Express has confirmed the news\n",
      "> Capital of India is going through great decline.\"\n",
      "\n",
      "Mr. Modi will likely do what is in his best interests for the country, as he will\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# q = \"Hello, I'm a language model,\"\n",
    "q = \"Capital of India is\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "x = tokens\n",
    "\n",
    "# generate!\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 itself is not that capable by the looks? But generation process is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results from HF Pytorch model weights and HF Flax model weights - Flax was failing initially(gibberish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of prepared JAX modules dict: 76\n",
      "loading weights from pretrained gpt: gpt2\n",
      "Length of pytorch state dict: 149\n",
      "Length of prepared JAX modules dict: 76\n",
      "Transposing  lm_head\n",
      "Checking wpe: True\n",
      "Checking wte: True\n",
      "Checking post token embedding + position embedding: True\n",
      "Checking block0 - layernorm 1: True\n",
      "Checking block0 - self attention: True\n",
      "Checking block0 - post residual: True\n",
      "Checking block0 - layernorm 2: True\n",
      "Checking block0 - MLP: True\n",
      "Checking block0 - post residual: True\n",
      "Checking block1 - layernorm 1: True\n",
      "Checking block1 - self attention: True\n",
      "Checking block1 - post residual: True\n",
      "Checking block1 - layernorm 2: True\n",
      "Checking block1 - MLP: True\n",
      "Checking block1 - post residual: True\n",
      "Checking block2 - layernorm 1: True\n",
      "Checking block2 - self attention: True\n",
      "Checking block2 - post residual: True\n",
      "Checking block2 - layernorm 2: True\n",
      "Checking block2 - MLP: True\n",
      "Checking block2 - post residual: True\n",
      "Checking block3 - layernorm 1: True\n",
      "Checking block3 - self attention: True\n",
      "Checking block3 - post residual: True\n",
      "Checking block3 - layernorm 2: True\n",
      "Checking block3 - MLP: True\n",
      "Checking block3 - post residual: True\n",
      "Checking block4 - layernorm 1: True\n",
      "Checking block4 - self attention: True\n",
      "Checking block4 - post residual: True\n",
      "Checking block4 - layernorm 2: True\n",
      "Checking block4 - MLP: True\n",
      "Checking block4 - post residual: True\n",
      "Checking block5 - layernorm 1: True\n",
      "Checking block5 - self attention: True\n",
      "Checking block5 - post residual: True\n",
      "Checking block5 - layernorm 2: True\n",
      "Checking block5 - MLP: True\n",
      "Checking block5 - post residual: True\n",
      "Checking block6 - layernorm 1: True\n",
      "Checking block6 - self attention: True\n",
      "Checking block6 - post residual: True\n",
      "Checking block6 - layernorm 2: True\n",
      "Checking block6 - MLP: True\n",
      "Checking block6 - post residual: True\n",
      "Checking block7 - layernorm 1: True\n",
      "Checking block7 - self attention: True\n",
      "Checking block7 - post residual: True\n",
      "Checking block7 - layernorm 2: True\n",
      "Checking block7 - MLP: True\n",
      "Checking block7 - post residual: True\n",
      "Checking block8 - layernorm 1: True\n",
      "Checking block8 - self attention: True\n",
      "Checking block8 - post residual: True\n",
      "Checking block8 - layernorm 2: True\n",
      "Checking block8 - MLP: True\n",
      "Checking block8 - post residual: True\n",
      "Checking block9 - layernorm 1: True\n",
      "Checking block9 - self attention: True\n",
      "Checking block9 - post residual: True\n",
      "Checking block9 - layernorm 2: True\n",
      "Checking block9 - MLP: True\n",
      "Checking block9 - post residual: True\n",
      "Checking block10 - layernorm 1: True\n",
      "Checking block10 - self attention: True\n",
      "Checking block10 - post residual: True\n",
      "Checking block10 - layernorm 2: True\n",
      "Checking block10 - MLP: True\n",
      "Checking block10 - post residual: True\n",
      "Checking block11 - layernorm 1: True\n",
      "Checking block11 - self attention: True\n",
      "Checking block11 - post residual: True\n",
      "Checking block11 - layernorm 2: True\n",
      "Checking block11 - MLP: True\n",
      "Checking block11 - post residual: True\n",
      "Checking post final layer norm: True\n",
      "Checking post final classifier: True\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# q = \"Hello, I'm a language model,\"\n",
    "q = \"Capital of India is\"\n",
    "tokens = enc.encode(q)\n",
    "tokens = jnp.expand_dims(jnp.array(tokens), axis=0)\n",
    "\n",
    "from jax_gpt2 import GPT\n",
    "model_flax = GPT.from_pretrained_flax('gpt2')  \n",
    "model_hf = GPT.from_pretrained('gpt2')\n",
    "\n",
    "model_hf._compare(model_flax, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Had a doubt if the weights loaded from HF Flax model were somehow not getting updated - not entirely sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "Length of prepared JAX modules dict: 76\n",
      "Checking h.0.attn.c_attn.bias: True\n",
      "Checking h.0.attn.c_attn.kernel: True\n",
      "Checking h.0.attn.c_proj.bias: True\n",
      "Checking h.0.attn.c_proj.kernel: False\n",
      "Checking h.0.ln_1.bias: True\n",
      "Checking h.0.ln_1.scale: True\n",
      "Checking h.0.ln_2.bias: True\n",
      "Checking h.0.ln_2.scale: True\n",
      "Checking h.0.mlp.c_fc.bias: True\n",
      "Checking h.0.mlp.c_fc.kernel: True\n",
      "Checking h.0.mlp.c_proj.bias: True\n",
      "Checking h.0.mlp.c_proj.kernel: True\n",
      "Checking h.1.attn.c_attn.bias: True\n",
      "Checking h.1.attn.c_attn.kernel: True\n",
      "Checking h.1.attn.c_proj.bias: True\n",
      "Checking h.1.attn.c_proj.kernel: False\n",
      "Checking h.1.ln_1.bias: True\n",
      "Checking h.1.ln_1.scale: True\n",
      "Checking h.1.ln_2.bias: True\n",
      "Checking h.1.ln_2.scale: True\n",
      "Checking h.1.mlp.c_fc.bias: True\n",
      "Checking h.1.mlp.c_fc.kernel: True\n",
      "Checking h.1.mlp.c_proj.bias: True\n",
      "Checking h.1.mlp.c_proj.kernel: True\n",
      "Checking h.10.attn.c_attn.bias: True\n",
      "Checking h.10.attn.c_attn.kernel: True\n",
      "Checking h.10.attn.c_proj.bias: True\n",
      "Checking h.10.attn.c_proj.kernel: False\n",
      "Checking h.10.ln_1.bias: True\n",
      "Checking h.10.ln_1.scale: True\n",
      "Checking h.10.ln_2.bias: True\n",
      "Checking h.10.ln_2.scale: True\n",
      "Checking h.10.mlp.c_fc.bias: True\n",
      "Checking h.10.mlp.c_fc.kernel: True\n",
      "Checking h.10.mlp.c_proj.bias: True\n",
      "Checking h.10.mlp.c_proj.kernel: True\n",
      "Checking h.11.attn.c_attn.bias: True\n",
      "Checking h.11.attn.c_attn.kernel: True\n",
      "Checking h.11.attn.c_proj.bias: True\n",
      "Checking h.11.attn.c_proj.kernel: False\n",
      "Checking h.11.ln_1.bias: True\n",
      "Checking h.11.ln_1.scale: True\n",
      "Checking h.11.ln_2.bias: True\n",
      "Checking h.11.ln_2.scale: True\n",
      "Checking h.11.mlp.c_fc.bias: True\n",
      "Checking h.11.mlp.c_fc.kernel: True\n",
      "Checking h.11.mlp.c_proj.bias: True\n",
      "Checking h.11.mlp.c_proj.kernel: True\n",
      "Checking h.2.attn.c_attn.bias: True\n",
      "Checking h.2.attn.c_attn.kernel: True\n",
      "Checking h.2.attn.c_proj.bias: True\n",
      "Checking h.2.attn.c_proj.kernel: False\n",
      "Checking h.2.ln_1.bias: True\n",
      "Checking h.2.ln_1.scale: True\n",
      "Checking h.2.ln_2.bias: True\n",
      "Checking h.2.ln_2.scale: True\n",
      "Checking h.2.mlp.c_fc.bias: True\n",
      "Checking h.2.mlp.c_fc.kernel: True\n",
      "Checking h.2.mlp.c_proj.bias: True\n",
      "Checking h.2.mlp.c_proj.kernel: True\n",
      "Checking h.3.attn.c_attn.bias: True\n",
      "Checking h.3.attn.c_attn.kernel: True\n",
      "Checking h.3.attn.c_proj.bias: True\n",
      "Checking h.3.attn.c_proj.kernel: False\n",
      "Checking h.3.ln_1.bias: True\n",
      "Checking h.3.ln_1.scale: True\n",
      "Checking h.3.ln_2.bias: True\n",
      "Checking h.3.ln_2.scale: True\n",
      "Checking h.3.mlp.c_fc.bias: True\n",
      "Checking h.3.mlp.c_fc.kernel: True\n",
      "Checking h.3.mlp.c_proj.bias: True\n",
      "Checking h.3.mlp.c_proj.kernel: True\n",
      "Checking h.4.attn.c_attn.bias: True\n",
      "Checking h.4.attn.c_attn.kernel: True\n",
      "Checking h.4.attn.c_proj.bias: True\n",
      "Checking h.4.attn.c_proj.kernel: False\n",
      "Checking h.4.ln_1.bias: True\n",
      "Checking h.4.ln_1.scale: True\n",
      "Checking h.4.ln_2.bias: True\n",
      "Checking h.4.ln_2.scale: True\n",
      "Checking h.4.mlp.c_fc.bias: True\n",
      "Checking h.4.mlp.c_fc.kernel: True\n",
      "Checking h.4.mlp.c_proj.bias: True\n",
      "Checking h.4.mlp.c_proj.kernel: True\n",
      "Checking h.5.attn.c_attn.bias: True\n",
      "Checking h.5.attn.c_attn.kernel: True\n",
      "Checking h.5.attn.c_proj.bias: True\n",
      "Checking h.5.attn.c_proj.kernel: False\n",
      "Checking h.5.ln_1.bias: True\n",
      "Checking h.5.ln_1.scale: True\n",
      "Checking h.5.ln_2.bias: True\n",
      "Checking h.5.ln_2.scale: True\n",
      "Checking h.5.mlp.c_fc.bias: True\n",
      "Checking h.5.mlp.c_fc.kernel: True\n",
      "Checking h.5.mlp.c_proj.bias: True\n",
      "Checking h.5.mlp.c_proj.kernel: True\n",
      "Checking h.6.attn.c_attn.bias: True\n",
      "Checking h.6.attn.c_attn.kernel: True\n",
      "Checking h.6.attn.c_proj.bias: True\n",
      "Checking h.6.attn.c_proj.kernel: False\n",
      "Checking h.6.ln_1.bias: True\n",
      "Checking h.6.ln_1.scale: True\n",
      "Checking h.6.ln_2.bias: True\n",
      "Checking h.6.ln_2.scale: True\n",
      "Checking h.6.mlp.c_fc.bias: True\n",
      "Checking h.6.mlp.c_fc.kernel: True\n",
      "Checking h.6.mlp.c_proj.bias: True\n",
      "Checking h.6.mlp.c_proj.kernel: True\n",
      "Checking h.7.attn.c_attn.bias: True\n",
      "Checking h.7.attn.c_attn.kernel: True\n",
      "Checking h.7.attn.c_proj.bias: True\n",
      "Checking h.7.attn.c_proj.kernel: False\n",
      "Checking h.7.ln_1.bias: True\n",
      "Checking h.7.ln_1.scale: True\n",
      "Checking h.7.ln_2.bias: True\n",
      "Checking h.7.ln_2.scale: True\n",
      "Checking h.7.mlp.c_fc.bias: True\n",
      "Checking h.7.mlp.c_fc.kernel: True\n",
      "Checking h.7.mlp.c_proj.bias: True\n",
      "Checking h.7.mlp.c_proj.kernel: True\n",
      "Checking h.8.attn.c_attn.bias: True\n",
      "Checking h.8.attn.c_attn.kernel: True\n",
      "Checking h.8.attn.c_proj.bias: True\n",
      "Checking h.8.attn.c_proj.kernel: False\n",
      "Checking h.8.ln_1.bias: True\n",
      "Checking h.8.ln_1.scale: True\n",
      "Checking h.8.ln_2.bias: True\n",
      "Checking h.8.ln_2.scale: True\n",
      "Checking h.8.mlp.c_fc.bias: True\n",
      "Checking h.8.mlp.c_fc.kernel: True\n",
      "Checking h.8.mlp.c_proj.bias: True\n",
      "Checking h.8.mlp.c_proj.kernel: True\n",
      "Checking h.9.attn.c_attn.bias: True\n",
      "Checking h.9.attn.c_attn.kernel: True\n",
      "Checking h.9.attn.c_proj.bias: True\n",
      "Checking h.9.attn.c_proj.kernel: False\n",
      "Checking h.9.ln_1.bias: True\n",
      "Checking h.9.ln_1.scale: True\n",
      "Checking h.9.ln_2.bias: True\n",
      "Checking h.9.ln_2.scale: True\n",
      "Checking h.9.mlp.c_fc.bias: True\n",
      "Checking h.9.mlp.c_fc.kernel: True\n",
      "Checking h.9.mlp.c_proj.bias: True\n",
      "Checking h.9.mlp.c_proj.kernel: True\n",
      "Checking ln_f.bias: True\n",
      "Checking ln_f.scale: True\n",
      "Checking wpe.embedding: True\n",
      "Checking wte.embedding: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel\n",
    "model = FlaxGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "from flax.core import unfreeze\n",
    "from flax.traverse_util import flatten_dict\n",
    "params = unfreeze(model.params['transformer'])\n",
    "params = flatten_dict(params, sep='.')\n",
    "\n",
    "from jax_gpt2 import GPT\n",
    "model_flax = GPT.from_pretrained_flax('gpt2')  \n",
    "jax_modules_dict = {}\n",
    "for module_pair in model_flax.iter_modules():\n",
    "    if type(module_pair[1]).__name__  in ['Block', 'CausalSelfAttention', 'GPT', 'MLP']:\n",
    "        continue\n",
    "    module_path = '.'.join([str(x) for x in module_pair[0]])\n",
    "    module = module_pair[1]\n",
    "    jax_modules_dict[module_path] = module\n",
    "\n",
    "for param in params:\n",
    "    t = param.split('.')[-1]    # Inner key Eg. wpe.embedding\n",
    "    jax_key = '.'.join(param.split('.')[:-1:])\n",
    "    if params[param].shape == jax_modules_dict[jax_key].__dict__[t].value.shape:\n",
    "        print(f\"Checking {param}: {jnp.allclose(params[param], jax_modules_dict[jax_key].__dict__[t].value)}\")\n",
    "    elif params[param].T.shape == jax_modules_dict[jax_key].__dict__[t].value.shape:\n",
    "        print(f\"Checking {param}: {jnp.allclose(params[param].T, jax_modules_dict[jax_key].__dict__[t].value)}\")\n",
    "    else:\n",
    "        print(f\"Shape mismatch for {param}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
